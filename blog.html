<!DOCTYPE html>

<html>
<head>
    <meta charset="utf-8">
    <title>清华大学-CSLT-自然语言处理研究组</title>
    
    <script src="http//cdn.jquerytools.org/1.2.7/full/jquery.tools.min.js"></script>
    <script src="js/jquery-1.7.1.js" type="text/javascript"></script>
    <script src="js/ajaxfileupload.js" type="text/javascript"></script>
    
    <link rel="stylesheet" href="reset.css" type="text/css">
    <link rel="stylesheet" href="style.css" type="text/css">
    <link href='http//fonts.googleapis.com/css?family=Source+Sans+Pro:200,400,600,700&subset=latin,latin-ext' rel='stylesheet' type='text/css'>

</head>

<body>
    
    <!-- *********  Header  ********** -->
    
    <div id="header">
        <div id="header_in">
        
        <h1><a href="index.html"><b>清华大学 CSLT </b>自然语言处理研究组</a></h1>
        
        <div id="menu">
         <ul>
            <li><a href="index.html">首页</a></li>
            <li><a href="about.html">团队</a></li>
            <li><a href="ourwork.html">DEMO</a></li>
            <li><a href="blog.html" class="active">成果</a></li>
            <li><a href="contact.html">招聘</a></li>            
         </ul>
        </div>
        
        </div>
    </div>
    
    <!-- *********  Main part – headline ********** -->
    
        
        <div id="main_part_inner">
            <div id="main_part_inner_in">
        
            <h2>研究成果</h2>
            
            <div class="button_main">
                <a href="download.html" class="button_dark">加入我们！</a>
            </div>
            
            </div>
            
        </div>
        
        
        <!-- *********  Content  ********** -->
        
        <div id="content_inner">
           <h3><b>Publication</b></h3>
           <div class="cara"></div>  
           
            <div>
               <div class="pub-left">
                  <h4>点击查看全文<br>
                <span>了解更多</span>
                </h4>
                  <a href="http://arxiv.org/pdf/1506.04940.pdf" alt="item4"><img src="img/1.jpg" alt="item4"></a>
               </div>
               <div class="pub-right">
                  <h4>Recognize Foreign Low-FrequencyWords with Similar Pairs</h4>
                  <p>Low-frequency words place a major challenge for automatic
                  speech recognition (ASR). The probabilities of these words,
                  which are often important name entities, are generally underestimated
                  by the language model (LM) due to their limited occurrences
                  in the training data. Recently, we proposed a wordpair
                  approach to deal with the problem, which borrows information
                  of frequent words to enhance the probabilities of lowfrequency
                  words. This paper presents an extension to the wordpair
                  method by involving multiple ‘predicting words’ to produce
                  better estimation for low-frequency words. We also employ
                  this approach to deal with out-of-language words in the
                  task of multi-lingual speech recognition.</p>
                 <p class="orange">Xi Ma, Xiaoxi Wang, Dong Wang</p>
                 <p class="blue">Interspeech 2015</p>
               </div>
           </div>
           <div class="cara"></div> 

             <div>
               <div class="pub-left">
                   <h4>点击查看全文<br>
                <span>了解更多</span>
                </h4>
                  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7230421"><img src="img/2.jpg" alt="item4"></a>
               </div>
               <div class="pub-right">
                  <h4>LOW-FREQUENCY WORD ENHANCEMENT WITH SIMILAR PAIRS IN SPEECH RECOGNITION</h4>
                  <p>In practical automatic speech recognition (ASR) systems, it
                  is difficult to recognize words that are with low-frequency
                  in the language model (LM) training data. Ironically, these
                  words tend to be highly important as they are often domainspecific
                  name entities. In order to meet this challenge, we
                  present a novel approach that enhances the weights of these
                  words by borrowing information from some high-frequency
                  words that are similar to the target words. Experimental results
                  demonstrated that our method can significantly improve
                  ASR performance on low-frequency words and does not impact
                  performance on high-frequency words. Additionally, this
                  method can be easily extended to deal with new words that are
                  absent in the LM training data.</p>
                 <p class="orange">Xi Ma, Xiaoxi Wang, Dong Wang</p>
                 <p class="blue">ChinaSIP 2015</p>
               </div>
           </div>
           <div class="cara"></div> 

            <div>
               <div class="pub-left">
                 <h4>点击查看全文<br>
                <span>了解更多</span>
                </h4>
                  <a href="http://xueshu.baidu.com/s?wd=paperuri%3A%28a44c015273ae7d3fad30d7e5859569ab%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6936669&ie=utf-8"><img src="img/3.jpg" alt="item4"></a>
               </div>
               <div class="pub-right">
                  <h4>Document Classification Based on Word Vectors</h4>
                  <p>This paper proposes a document classification approach based on word vectors. By learning context relationships, word vectors may represent fine-grained semantic elements. We assume that these low-level semantic implementation of words can be composed to represent high-level semantic concepts, and thus the semantic content of a document can be derived from those of the words it involves. Our experiments confirm that, even with the simplest pooling method, the document representation based on word vectors can deliver good performance on text classification tasks. When compared to the conventional LDA-based approach, the word vector approach is more stable, efficient and generalizable.</p>
                 <p class="orange">Rong Liu, Dong Wang, Chao Xing</p>
                 <p class="blue">Tsinghua Xuebao (in Chinese)</p>
               </div>
           </div>
           <div class="cara"></div> 

         <div>
               <div class="pub-left">
                 <h4>点击查看全文<br>
                <span>了解更多</span>
                </h4>
                  <a href="http://arxiv.org/pdf/1511.00271v1.pdf"><img src="img/4.jpg" alt="item4"></a>
               </div>
               <div class="pub-right">
                  <h4>Stochastic Top-k ListNet</h4>
                  <p>ListNet is a well-known listwise learning
                    to rank model and has gained much attention
                    in recent years. A particular problem
                    of ListNet, however, is the high computation
                    complexity in model training, mainly
                    due to the large number of object permutations
                    involved in computing the gradients.
                    This paper proposes a stochastic
                    ListNet approach which computes the gradient
                    within a bounded permutation subset.
                    It significantly reduces the computation
                    complexity of model training and
                    allows extension to Top-k models, which
                    is impossible with the conventional implementation
                    based on full-set permutations.
                    Meanwhile, the new approach utilizes
                    partial ranking information of human
                    labels, which helps improve model quality.
                    Our experiments demonstrated that the
                    stochastic ListNet method indeed leads to
                    better ranking performance and speeds up
                    the model training remarkably.
                    </p>
                 <p class="orange">Tianyi Luo, Dong Wang, Rong Liu, Yiqiao Pan</p>
                 <p class="blue">EMNLP 2015 long oral paper</p>
               </div>
           </div>
           <div class="cara"></div> 

           <div>
               <div class="pub-left">
                 <h4>点击查看全文<br>
                <span>了解更多</span>
                </h4>
                  <a href="http://anthology.aclweb.org/W/W15/W15-4004.pdf"><img src="img/5.jpg" alt="item4"></a>
               </div>
               <div class="pub-right">
                  <h4>Joint Semantic Relevance Learning with Text Data and Graph Knowledge</h4>
                  <p>Inferring semantic relevance among enti- ties (e.g., entries of Wikipedia) is impor- tant and challenging. According to the in- formation resources, the inference can be categorized into learning with either raw text data, or labeled text data (e.g., wik- i page), or graph knowledge (e.g, Word- Net). Although graph knowledge tends to be more reliable, text data is much less costly and offers a better coverage.
                  We show in this paper that different re- sources are complementary and can be combined to improve semantic learning. Particularly, we present a joint learning ap- proach that learns vectors of entities by leveraging resources of both text data and graph knowledge. The experiments con- ducted on the semantic relatedness task show that text-based learning works well on general domain tasks, however for tasks in specific domains, joint learning that in- volves both text data and graph knowledge offers significant improvement.</p>
                 <p class="orange">Dongxu Zhang, Dong Wang, Rong Liu</p>
                 <p class="blue">ACL 2015, Workshop CVSC</p>
               </div>
           </div>
           <div class="cara"></div> 

        <div>
               <div class="pub-left">
                 <h4>点击查看全文<br>
                <span>了解更多</span>
                </h4>
                  <a href="http://www.aclweb.org/anthology/N/N15/N15-1104.pdf"><img src="img/6.jpg" alt="item4"></a>
               </div>
               <div class="pub-right">
                  <h4>Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation</h4>
                  <p>Word embedding has been found to be highly
                    powerful to translate words from one language
                    to another by a simple linear transform.
                    However, we found some inconsistence
                    among the objective functions of the embedding
                    and the transform learning, as well as
                    the distance measurement. This paper proposes
                    a solution which normalizes the word vectors
                    on a hypersphere and constrains the linear
                    transform as an orthogonal transform. The
                    experimental results confirmed that the proposed
                    solution can offer better performance
                    on a word similarity task and an English-toSpanish
                    word translation task</p>
                 <p class="orange">Chao Xing, Dong Wang, Chao Liu, Yiye Lin,</p>
                 <p class="blue">NAACL 2015</p>
               </div>
           </div>
           <div class="cara"></div> 

           <div>
               <div class="pub-left">
                 <h4>点击查看全文<br>
                <span>了解更多</span>
                </h4>
                  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7041633"><img src="img/7.jpg" alt="item4"></a>
               </div>
               <div class="pub-right">
                  <h4>Document Classification with Distributions of Word Vectors</h4>
                  <p>The word-to-vector (W2V) technique represents words as low-dimensional continuous vectors in such a way that semantic related words are close to each other. This produces a semantic space where a word or a word collection (e.g., a document) can be well represented, and thus lends itself to a multitude of applications including document classification. Our previous study demonstrated that representations derived from word vectors are highly promising in document classification and can deliver better performance than the conventional LDA model. This paper extends the previous research and proposes to model distributions of word vectors in documents or document classes. This extends the naive approach to deriving document representations by average pooling and explores the possibility of modeling documents in the semantic space. Experiments on the sohu text database confirmed that the new approach may produce better performance on document classification.</p>
                 <p class="orange">Chao Xing, Dong Wang, Xuwei Zhang, Chao Liu</p>
                 <p class="blue"> APSIPA 2014</p>
               </div>
           </div>
           <div class="cara"></div> 

                <div>
               <div class="pub-left">
                 <h4>点击查看全文<br>
                <span>了解更多</span>
                </h4>
                  <a href="http://cslt.riit.tsinghua.edu.cn/mediawiki/images/a/a4/Vmclass.pdf"><img src="img/8.jpg" alt="item4"></a>
               </div>
               <div class="pub-right">
                  <h4>Document Classification with Spherical Word Vectors</h4>
                  <p>Recent research shows that low-dimensional continuous representations of
                    words (word vectors) can be successfully employed to classify documents,
                    and document vectors derived from semantic clustering work better than
                    those derived from simple average pooling. On the other hand, our recent
                    study demonstrated that embedding words on a hypersphere offers better
                    performance on tasks including semantic relatedness and bilingual
                    translation when compared to the original approach that embeds words in an
                    unconstrained plane space. In this paper, spherical word vectors are applied
                    to the document classification task. The experiments show that spherical
                    word vectors can deliver good performance when combined with semantic
                    clustering based on vMF distributions</p>
                 <p class="orange">Yiqiao Pan, Chao Xing, Dong Wang</p>
                 <p class="blue">APSIPA 2015</p>
               </div>
           </div>
    
    
    <!-- *********  Footer  ********** -->
    
    <hr class="cleanit">
       <div id="footer">
        <div id="footer_in">
            <p><a href="http://cslt.riit.tsinghua.edu.cn/">@版权所有:清华大学信息技术研究院语音和语言技术中心</a> &mdash; 地址:北京市海淀区清华大学FIT楼1-303房间</p>
            <p>电话/传真:010-62796589  &nbsp&nbsp&nbsp&nbsp邮编:100084  &nbsp&nbsp&nbsp&nbspE-mail:cslt@mail.tsinghua.edu.cn</p>
        </div>
    </div>
         
<script>
// script for testimonial' tabs
$(function() {
    $("ul.controls").tabs("div.testimonials > div");
});
</script>

</body>
</html>
